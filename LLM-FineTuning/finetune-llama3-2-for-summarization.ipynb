{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U torch==\"2.4.0\" bitsandbytes datasets transformers accelerate wandb trl peft evaluate rouge_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:58:31.573218Z","iopub.execute_input":"2024-11-24T03:58:31.573578Z","iopub.status.idle":"2024-11-24T03:58:40.722730Z","shell.execute_reply.started":"2024-11-24T03:58:31.573542Z","shell.execute_reply":"2024-11-24T03:58:40.721715Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Setup Initialization","metadata":{}},{"cell_type":"code","source":"import os\nimport wandb\nimport torch\nimport bitsandbytes as bnb\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom datasets import load_dataset\nfrom functools import partial\nfrom transformers import (\n    BitsAndBytesConfig, \n    AutoModelForCausalLM, \n    AutoTokenizer, \n    TrainingArguments\n)\nfrom peft import (\n    LoraConfig, \n    prepare_model_for_kbit_training, \n    get_peft_model, \n    PeftModel\n)\nfrom trl import SFTTrainer\nimport evaluate\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:58:42.775577Z","iopub.execute_input":"2024-11-24T03:58:42.776501Z","iopub.status.idle":"2024-11-24T03:58:50.850007Z","shell.execute_reply.started":"2024-11-24T03:58:42.776457Z","shell.execute_reply":"2024-11-24T03:58:50.849021Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"WANDB_PROJECT\"] = 'llm-finetuning-for-summarisation'\nos.environ[\"WANDB_LOG_MODEL\"] = 'end'\nos.environ[\"WANDB_WATCH\"] = 'false'\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:58:50.851629Z","iopub.execute_input":"2024-11-24T03:58:50.852226Z","iopub.status.idle":"2024-11-24T03:58:50.856923Z","shell.execute_reply.started":"2024-11-24T03:58:50.852193Z","shell.execute_reply":"2024-11-24T03:58:50.855899Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"wandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:58:50.858048Z","iopub.execute_input":"2024-11-24T03:58:50.858331Z","iopub.status.idle":"2024-11-24T03:58:55.889129Z","shell.execute_reply.started":"2024-11-24T03:58:50.858305Z","shell.execute_reply":"2024-11-24T03:58:55.888387Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"working on {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:58:55.890912Z","iopub.execute_input":"2024-11-24T03:58:55.891561Z","iopub.status.idle":"2024-11-24T03:58:55.896639Z","shell.execute_reply.started":"2024-11-24T03:58:55.891531Z","shell.execute_reply":"2024-11-24T03:58:55.895761Z"}},"outputs":[{"name":"stdout","text":"working on cuda:0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:59:01.183119Z","iopub.execute_input":"2024-11-24T03:59:01.183496Z","iopub.status.idle":"2024-11-24T03:59:01.188540Z","shell.execute_reply.started":"2024-11-24T03:59:01.183458Z","shell.execute_reply":"2024-11-24T03:59:01.187272Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"huggingface_dataset = 'neil-code/dialogsum-test'\ndataset = load_dataset(huggingface_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:59:10.704065Z","iopub.execute_input":"2024-11-24T03:59:10.704453Z","iopub.status.idle":"2024-11-24T03:59:12.783368Z","shell.execute_reply.started":"2024-11-24T03:59:10.704420Z","shell.execute_reply":"2024-11-24T03:59:12.782586Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:59:13.341011Z","iopub.execute_input":"2024-11-24T03:59:13.341352Z","iopub.status.idle":"2024-11-24T03:59:13.348024Z","shell.execute_reply.started":"2024-11-24T03:59:13.341324Z","shell.execute_reply":"2024-11-24T03:59:13.346941Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# QLORA Setup","metadata":{}},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:59:14.760030Z","iopub.execute_input":"2024-11-24T03:59:14.760780Z","iopub.status.idle":"2024-11-24T03:59:14.766423Z","shell.execute_reply.started":"2024-11-24T03:59:14.760739Z","shell.execute_reply":"2024-11-24T03:59:14.765636Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"model_name = \"NousResearch/Llama-3.2-1B\"\n\noriginal_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\n\noriginal_model.config.use_cache = False\noriginal_model.config.pretraining_tp = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T03:59:26.111759Z","iopub.execute_input":"2024-11-24T03:59:26.112841Z","iopub.status.idle":"2024-11-24T04:00:38.124882Z","shell.execute_reply.started":"2024-11-24T03:59:26.112636Z","shell.execute_reply":"2024-11-24T04:00:38.123961Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   5%|5         | 126M/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2fd78d834ab47a79d71763e5e5d32a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/186 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79d829cc0b8c483aaf4ed469335d5dc4"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# Load Tokenizer","metadata":{}},{"cell_type":"code","source":"max_seq_length = 512\ntokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length, padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:38.126463Z","iopub.execute_input":"2024-11-24T04:00:38.126776Z","iopub.status.idle":"2024-11-24T04:00:41.042219Z","shell.execute_reply.started":"2024-11-24T04:00:38.126747Z","shell.execute_reply":"2024-11-24T04:00:41.041214Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a5112aa4275477cadcf2f47bdfaa9d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f739a81123cc4524855ce3ce5187fcbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e05265d06782422e99252d9369b04f6e"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# Function to Generate LLM Response","metadata":{}},{"cell_type":"code","source":"def generate(model, prompt, max_length, tokenizer):\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=512\n    ).to(model.device)\n\n    outputs = model.generate(\n        inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_new_tokens=max_length,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n        num_beams=5,\n        temperature=0.1,\n        top_p=0.9,\n        repetition_penalty=1.2\n    )\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:41.043523Z","iopub.execute_input":"2024-11-24T04:00:41.043918Z","iopub.status.idle":"2024-11-24T04:00:41.049292Z","shell.execute_reply.started":"2024-11-24T04:00:41.043874Z","shell.execute_reply":"2024-11-24T04:00:41.048485Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Zero-Shot Response","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 47\nset_seed(seed)\n\nindex = 10\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = generate(original_model,formatted_prompt,100,tokenizer)\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\nprint(dash_line)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:41.051879Z","iopub.execute_input":"2024-11-24T04:00:41.052497Z","iopub.status.idle":"2024-11-24T04:00:46.863136Z","shell.execute_reply.started":"2024-11-24T04:00:41.052449Z","shell.execute_reply":"2024-11-24T04:00:46.862213Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n\n---------------------------------------------------------------------------------------------------\nCPU times: user 5.19 s, sys: 132 ms, total: 5.32 s\nWall time: 5.8 s\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Functions to Process Dataset for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:46.864360Z","iopub.execute_input":"2024-11-24T04:00:46.864676Z","iopub.status.idle":"2024-11-24T04:00:46.870463Z","shell.execute_reply.started":"2024-11-24T04:00:46.864623Z","shell.execute_reply":"2024-11-24T04:00:46.869550Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:46.871404Z","iopub.execute_input":"2024-11-24T04:00:46.871719Z","iopub.status.idle":"2024-11-24T04:00:46.882875Z","shell.execute_reply.started":"2024-11-24T04:00:46.871692Z","shell.execute_reply":"2024-11-24T04:00:46.882124Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:46.883816Z","iopub.execute_input":"2024-11-24T04:00:46.884169Z","iopub.status.idle":"2024-11-24T04:00:46.893144Z","shell.execute_reply.started":"2024-11-24T04:00:46.884142Z","shell.execute_reply":"2024-11-24T04:00:46.892185Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:46.893961Z","iopub.execute_input":"2024-11-24T04:00:46.894235Z","iopub.status.idle":"2024-11-24T04:00:46.907326Z","shell.execute_reply.started":"2024-11-24T04:00:46.894209Z","shell.execute_reply":"2024-11-24T04:00:46.906514Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Preprocess Dataset","metadata":{}},{"cell_type":"code","source":"max_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:17:07.662879Z","iopub.execute_input":"2024-11-24T05:17:07.663446Z","iopub.status.idle":"2024-11-24T05:17:07.840785Z","shell.execute_reply.started":"2024-11-24T05:17:07.663413Z","shell.execute_reply":"2024-11-24T05:17:07.839840Z"}},"outputs":[{"name":"stdout","text":"Found max lenth: 131072\n131072\nPreprocessing dataset...\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"# Setup PEFT Config","metadata":{}},{"cell_type":"code","source":"original_model = prepare_model_for_kbit_training(original_model)\noriginal_model.gradient_checkpointing_enable()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:48.846907Z","iopub.execute_input":"2024-11-24T04:00:48.847192Z","iopub.status.idle":"2024-11-24T04:00:48.855549Z","shell.execute_reply.started":"2024-11-24T04:00:48.847163Z","shell.execute_reply":"2024-11-24T04:00:48.854941Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"output_dir=\"trained_weights\"\n\npeft_config = LoraConfig(\n    lora_alpha=32,\n    lora_dropout=0.1,\n    r=128,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n)\n\npeft_model = get_peft_model(original_model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:48.856430Z","iopub.execute_input":"2024-11-24T04:00:48.856714Z","iopub.status.idle":"2024-11-24T04:00:50.010396Z","shell.execute_reply.started":"2024-11-24T04:00:48.856675Z","shell.execute_reply":"2024-11-24T04:00:50.009581Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Setup SFTTrainer","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    save_steps=100,\n    logging_steps=25, \n    learning_rate=2e-5,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    fp16_opt_level=\"O1\",\n    max_grad_norm=0.3,\n    max_steps=1000,\n    warmup_ratio=0.03,\n    group_by_length=False,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"wandb\",\n)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=training_arguments,\n    train_dataset=train_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=max_seq_length,\n    packing=False,\n    dataset_kwargs={\n        \"add_special_tokens\": False,\n        \"append_concat_token\": False,\n    },\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:50.012026Z","iopub.execute_input":"2024-11-24T04:00:50.012405Z","iopub.status.idle":"2024-11-24T04:00:50.078840Z","shell.execute_reply.started":"2024-11-24T04:00:50.012372Z","shell.execute_reply":"2024-11-24T04:00:50.078009Z"}},"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Train the Model","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T04:00:50.079955Z","iopub.execute_input":"2024-11-24T04:00:50.080245Z","iopub.status.idle":"2024-11-24T05:09:03.625802Z","shell.execute_reply.started":"2024-11-24T04:00:50.080214Z","shell.execute_reply":"2024-11-24T05:09:03.624964Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvarundixit4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241124_040050-3s80a5ki</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/varundixit4/llm-finetuning-for-summarisation/runs/3s80a5ki' target=\"_blank\">trained_weights</a></strong> to <a href='https://wandb.ai/varundixit4/llm-finetuning-for-summarisation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/varundixit4/llm-finetuning-for-summarisation' target=\"_blank\">https://wandb.ai/varundixit4/llm-finetuning-for-summarisation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/varundixit4/llm-finetuning-for-summarisation/runs/3s80a5ki' target=\"_blank\">https://wandb.ai/varundixit4/llm-finetuning-for-summarisation/runs/3s80a5ki</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 1:08:06, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.255000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.906100</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.660200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.564200</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.580800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.570400</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.535000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.516900</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.550700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.528800</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.497600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.492700</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.499900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.508200</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.514900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.481200</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.526300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.526000</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.488900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.516400</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.456700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.458000</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>1.509900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.475800</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>1.516000</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.447800</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>1.477000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.441300</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>1.466700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.500000</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>1.460300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.457000</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>1.479500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.420200</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>1.447800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.450000</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>1.487800</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.471600</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>1.431800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.510000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=1.5271420249938965, metrics={'train_runtime': 4088.103, 'train_samples_per_second': 1.957, 'train_steps_per_second': 0.245, 'total_flos': 1.339379480199168e+16, 'train_loss': 1.5271420249938965, 'epoch': 4.00200100050025})"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"# Inference of Trained Model","metadata":{}},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                device_map=device,\n                                                torch_dtype=compute_dtype,\n                                                quantization_config=bnb_config,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:09:44.468516Z","iopub.execute_input":"2024-11-24T05:09:44.469298Z","iopub.status.idle":"2024-11-24T05:09:49.637356Z","shell.execute_reply.started":"2024-11-24T05:09:44.469262Z","shell.execute_reply":"2024-11-24T05:09:49.636592Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:09:49.638974Z","iopub.execute_input":"2024-11-24T05:09:49.639293Z","iopub.status.idle":"2024-11-24T05:09:50.309688Z","shell.execute_reply.started":"2024-11-24T05:09:49.639265Z","shell.execute_reply":"2024-11-24T05:09:50.308980Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"ft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/trained_weights/checkpoint-900\", torch_dtype=torch.float16, is_trainable=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:09:51.071201Z","iopub.execute_input":"2024-11-24T05:09:51.071546Z","iopub.status.idle":"2024-11-24T05:09:52.392806Z","shell.execute_reply.started":"2024-11-24T05:09:51.071515Z","shell.execute_reply":"2024-11-24T05:09:52.392096Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 10\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = generate(ft_model,prompt,100,eval_tokenizer)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\nprefix, success, result = peft_model_output.partition('Instruct')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:14:11.257606Z","iopub.execute_input":"2024-11-24T05:14:11.258004Z","iopub.status.idle":"2024-11-24T05:14:18.386836Z","shell.execute_reply.started":"2024-11-24T05:14:11.257969Z","shell.execute_reply":"2024-11-24T05:14:18.385944Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\n#Person1# and #Person2# are celebrating Brian's birthday. #Person1# invites #Person2# to have a dance with him. #Person2# thinks #Person1# looks great and says #Person1# is glowing. #Person1# invites #Person2# to have a drink together to celebrate Brian's birthday.\n\nCPU times: user 7.05 s, sys: 77.3 ms, total: 7.13 s\nWall time: 7.12 s\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# Evaluation using ROGUE Score","metadata":{}},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device,\n                                                      torch_dtype=compute_dtype,\n                                                      quantization_config=bnb_config,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:09:59.483723Z","iopub.execute_input":"2024-11-24T05:09:59.484012Z","iopub.status.idle":"2024-11-24T05:10:02.944599Z","shell.execute_reply.started":"2024-11-24T05:09:59.483984Z","shell.execute_reply":"2024-11-24T05:10:02.943705Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = generate(original_model,prompt,100,eval_tokenizer)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = generate(ft_model,prompt,100,eval_tokenizer)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    peft_model_text_output, success, result = peft_model_output.partition('Instruct')\n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:14:37.511233Z","iopub.execute_input":"2024-11-24T05:14:37.511596Z","iopub.status.idle":"2024-11-24T05:16:40.779207Z","shell.execute_reply.started":"2024-11-24T05:14:37.511561Z","shell.execute_reply":"2024-11-24T05:16:40.778397Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)\n\nprint(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:16:40.780751Z","iopub.execute_input":"2024-11-24T05:16:40.781059Z","iopub.status.idle":"2024-11-24T05:16:42.536687Z","shell.execute_reply.started":"2024-11-24T05:16:40.781028Z","shell.execute_reply":"2024-11-24T05:16:42.535866Z"}},"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2895314607826919, 'rouge2': 0.09058111562913862, 'rougeL': 0.20996585282618457, 'rougeLsum': 0.2361721138974836}\nPEFT MODEL:\n{'rouge1': 0.3716440998224132, 'rouge2': 0.13114848449287808, 'rougeL': 0.2831427991671398, 'rougeLsum': 0.2829204192496464}\nAbsolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 8.21%\nrouge2: 4.06%\nrougeL: 7.32%\nrougeLsum: 4.67%\n","output_type":"stream"}],"execution_count":39}]}